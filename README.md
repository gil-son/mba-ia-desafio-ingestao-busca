# Desafio MBA Engenharia de Software com IA - Full Cycle

# ğŸ“‘ IngestÃ£o e Busca SemÃ¢ntica com LangChain e Postgres

Este projeto Ã© uma implementaÃ§Ã£o de um sistema de **RAG (Retrieval-Augmented Generation)**. Ele permite a ingestÃ£o de documentos PDF em um banco de dados vetorial PostgreSQL (utilizando a extensÃ£o `pgVector`) e a realizaÃ§Ã£o de perguntas via linha de comando (CLI) com respostas baseadas exclusivamente no conteÃºdo do documento.

## ğŸš€ Tecnologias Utilizadas

* **Linguagem:** Python 3.11+
* **Framework LLM:** [LangChain](https://www.langchain.com/)
* **Banco de Dados Vetorial:** PostgreSQL com a extensÃ£o [pgVector](https://github.com/pgvector/pgvector)
* **Modelos de IA (Google Gemini):**
    * Embeddings: `models/embedding-001`
    * LLM: `gemini-2.5-flash-lite`
* **Infraestrutura:** Docker & Docker Compose

## ğŸ“‚ Estrutura do Projeto

```text
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py         # Script para processar o PDF e salvar no banco
â”‚   â”œâ”€â”€ search.py         # LÃ³gica de busca vetorial e integraÃ§Ã£o com LLM
â”‚   â”œâ”€â”€ chat.py           # Interface de linha de comando (CLI)
â”œâ”€â”€ document.pdf          # PDF que serÃ¡ processado
â”œâ”€â”€ docker-compose.yml    # ConfiguraÃ§Ã£o do banco de dados PostgreSQL
â”œâ”€â”€ .env                  # VariÃ¡veis de ambiente (API Keys, DB URL)
â”œâ”€â”€ requirements.txt      # DependÃªncias do projeto
â””â”€â”€ README.md             # InstruÃ§Ãµes de uso
```

## ğŸ› ï¸ ConfiguraÃ§Ã£o e InstalaÃ§Ã£o
1. Clonar o RepositÃ³rio

```
git clone <url-do-seu-repositorio>
cd <nome-da-pasta>
```

2. Ambiente Virtual e DependÃªncias
Crie um ambiente virtual e instale as bibliotecas necessÃ¡rias:

```
python3 -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
pip install -r requirements.txt
```

3. VariÃ¡veis de Ambiente
Crie um arquivo .env na raiz do projeto seguindo o modelo abaixo:

```
GOOGLE_API_KEY='sua_chave_aqui'
DATABASE_URL='postgresql+psycopg://postgres:postgres@localhost:5432/rag'
PG_VECTOR_COLLECTION='document_collection'
PDF_PATH='document.pdf'
EMBED_MODEL='models/embedding-001'
GEMINI_LLM_MODEL='gemini-2.5-flash-lite'
```

### ğŸƒ Como Executar
Siga a ordem abaixo para garantir que o sistema funcione corretamente:

1. Subir o Banco de Dados
Certifique-se de que o Docker estÃ¡ rodando e execute:

```
docker compose up -d
```

2. IngestÃ£o do PDF
Este comando lerÃ¡ o arquivo document.pdf, farÃ¡ a quebra em pedaÃ§os (chunks) de 1000 caracteres e salvarÃ¡ os vetores no banco:

```
python src/ingest.py
```

3. Iniciar o Chat
Agora vocÃª pode interagir com o documento via CLI:

```
python src/chat.py
```


ğŸ§  Regras de Resposta do Sistema
O sistema foi configurado para ser rigoroso conforme as diretrizes do desafio:

- Fidelidade ao Contexto: Ele sÃ³ responde o que estiver no PDF.

- Sem AlucinaÃ§Ãµes: Se a informaÃ§Ã£o nÃ£o for encontrada, a resposta padrÃ£o serÃ¡: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

- Sem Conhecimento Externo: O modelo ignora fatos externos se nÃ£o estiverem no texto.


ğŸ› ï¸ SoluÃ§Ã£o de Problemas
- Erro de conexÃ£o com o banco: Verifique se o container Docker estÃ¡ em execuÃ§Ã£o (docker ps).

- Erro de API Key: Verifique se sua chave do Google AI Studio possui cotas disponÃ­veis.